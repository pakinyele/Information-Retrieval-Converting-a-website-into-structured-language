{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading All the libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#!{sys.executable} -m spacy download en\n",
    "import numpy as np\n",
    "import requests\n",
    "import nltk\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTF-8 Coding \n",
    "from spacy import displacy\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from requests import get\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import brown\n",
    "#brown.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENGINEERING A COMPLETE SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the URLS\n",
    "url1 = 'http://www.multimediaeval.org/mediaeval2019/memorability/'\n",
    "url2= 'https://sites.google.com/view/siirh2020/'\n",
    "url =[url1 , url2]\n",
    "docid = 0\n",
    "document = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function(parsing) for Formatting HTML parsing and returning text\n",
    "def parsing(url):\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html5lib') #creating a parse tree for parsed pages that are used to extract data from HTML\n",
    "    for script in soup([\"script\", \"style\", 'aside']):\n",
    "        script.extract()\n",
    "    text = \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =[]\n",
    "for i in url:\n",
    "    text.append(parsing(i))\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output of HTML parsing into a file\n",
    "with open('Htmlparsing1.txt', 'w') as f:\n",
    "    print(text[0], file = f)\n",
    "with open('Htmlparsing2.txt', 'w') as f:\n",
    "    print(text[1], file = f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function(text_processing) for Pre-processing \n",
    "def text_processing(text):\n",
    "    text = text.lower()\n",
    "    #Removing stop words and lemmatization \n",
    "    stop_words = set(stopwords.words('english')) #removing english words which do not add much meaning to the sentence.\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    new_words = [lemmatiser.lemmatize(word) for word in words if word not in stop_words]\n",
    "    new_words = ' '.join(map(str, new_words))\n",
    "    #new_words\n",
    "    return new_words\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing the text itself\n",
    "preprocessed_texta = text_processing(text[0])\n",
    "preprocessed_textb = text_processing(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output of preprocessed data into a file\n",
    "with open('preprocessed_texta.txt', 'w') as f:\n",
    "    print(preprocessed_texta, file = f)\n",
    "with open('preprocessed_textb.txt', 'w') as f:\n",
    "    print(preprocessed_texta, file = f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the function(tokenize) for word tokenization \n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Tokenization\n",
    "wordtokens = []\n",
    "for index in text:\n",
    "    wordtokens.append(tokenize(index))\n",
    "#wordtokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output for word tokenization into a file\n",
    "with open('word_tokenizationA.txt', 'w') as f:\n",
    "    print(wordtokens[0], file = f)\n",
    "with open('word_tokenizationB.txt', 'w') as f:\n",
    "    print(wordtokens[1], file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function for Sentence tokenization \n",
    "def tokenize_sentence(text):\n",
    "    words = sent_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence Tokenization\n",
    "sentencetokens = []\n",
    "for index in text:\n",
    "    sentencetokens.append(tokenize_sentence(index)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output of sentence tokenization into a file\n",
    "with open('sentence_tokenizationA.txt', 'w') as f:\n",
    "    print(sentencetokens[0], file = f)\n",
    "with open('sentence_tokenizationB.txt', 'w') as f:\n",
    "    print(sentencetokens[1], file = f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Sentence Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence Detection A. \n",
    "\n",
    "about_text = (preprocessed_texta)\n",
    "about_doc = nlp(about_text)\n",
    "sentencesA = list(about_doc.sents)\n",
    "len(sentencesA)\n",
    "\n",
    "#for sentence in sentencesA:\n",
    "     #print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output of the sentence detection into a file\n",
    "with open('sentence_detectionA.txt', 'w') as f:\n",
    "    print(sentencetokens[0], file = f)\n",
    "with open('sentence_detectionB.txt', 'w') as f:\n",
    "    print(sentencetokens[1], file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentence Detection B. \n",
    "\n",
    "about_text = (preprocessed_textb)\n",
    "about_doc = nlp(about_text)\n",
    "sentencesB = list(about_doc.sents)\n",
    "len(sentencesB)\n",
    "\n",
    "#for sentence in sentencesB:\n",
    "     #print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART OF SPEECH TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a function(pos_tagging) for Part Of Speech tagging\n",
    "def pos_tagging(text):\n",
    "    words = word_tokenize(text)\n",
    "    return pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part Of Speech tagging\n",
    "postags = []\n",
    "for i in text:\n",
    "    postags.append(pos_tagging(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the output of the Part of Speech Tagging into a file\n",
    "with open('PartOfSpeech_taggingA.txt', 'w') as f:\n",
    "    print(postags[0], file=f)\n",
    "with open('PartOfSpeech_taggingB.txt', 'w') as f:\n",
    "    print(postags[1], file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECTING KEYWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passing a function (corpus) to join both texta and textb\n",
    "corpus = [preprocessed_texta,preprocessed_textb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('corpus.txt', 'w') as f:\n",
    "    print(corpus, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indexing\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in doc 1\n",
      "['memorability' 'video' 'multimedia' '2018' 'predicting' 'term' 'make'\n",
      " 'memorable' 'model' 'data' '2019' 'demarty' 'ieee' 'score' 'duong']\n",
      "\n",
      "\n",
      "Top words in doc 2\n",
      "['university' 'health' 'national' 'content' 'paper' 'workshop' 'ir'\n",
      " 'medicine' 'related' 'lasige' 'mesinesp' 'ncbi' 'bsc' 'session' 'nih']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#70 Most frequent words\n",
    "n = 15\n",
    "for i in range(0,2):\n",
    "    feature_array = np.array(vectorizer.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(X[i].toarray()).flatten()[::-1]\n",
    "    top_n = feature_array[tfidf_sorting][:n]\n",
    "    print(\"Top words in doc {}\".format(i+1))\n",
    "    print(top_n)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Most frequent words output into a file\n",
    "with open('Most_Frequent_words.txt', 'w') as f:\n",
    "    print(top_n, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the feature scores in the documents\n",
    "F_scores= dict(zip(top_n,X.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'university': 0.019706471638525257,\n",
       " 'health': 0.019706471638525257,\n",
       " 'national': 0.014021316875700518,\n",
       " 'content': 0.014021316875700518,\n",
       " 'paper': 0.019706471638525257,\n",
       " 'workshop': 0.019706471638525257,\n",
       " 'ir': 0.019706471638525257,\n",
       " 'medicine': 0.014021316875700518,\n",
       " 'related': 0.019706471638525257,\n",
       " 'lasige': 0.03941294327705051,\n",
       " 'mesinesp': 0.019706471638525257,\n",
       " 'ncbi': 0.028042633751401036,\n",
       " 'bsc': 0.019706471638525257,\n",
       " 'session': 0.019706471638525257,\n",
       " 'nih': 0.019706471638525257}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Feature scores output into a file\n",
    "#F_scores.to_csv(\"/Users/preciousakinyele/Documents/SPRING/Feature_Scores.csv\", sep=\",\")\n",
    "with open('Feature_scores.txt', 'w') as f:\n",
    "    print(F_scores, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING OR MORPHOLOGICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = nlp(text[0])\n",
    "documentB = nlp(text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documentB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_listdocA = []\n",
    "\n",
    "for ent in documentA.ents:\n",
    "    entity_listdocA.append([ent, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_listdocB = []\n",
    "\n",
    "for ent in documentB.ents:\n",
    "    entity_listdocB.append([ent, ent.label_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('STEMA.txt', 'w') as f:\n",
    "    print(entity_listdocA, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving output to a file\n",
    "with open('STEMB.txt', 'w') as f:\n",
    "    print(entity_listdocB, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
